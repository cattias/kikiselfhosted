# ceph/ceph-pool-fix-job.yaml

apiVersion: batch/v1
kind: Job
metadata:
  name: ceph-system-pool-fix
  namespace: rook-ceph # Must be in the rook-ceph namespace
  annotations:
    # ArgoCD Post-Sync Hook Annotation
    argocd.argoproj.io/hook: PostSync
    argocd.argoproj.io/hook-delete-policy: HookSucceeded
spec:
  template:
    spec:
      # Use a ServiceAccount with permissions to run 'kubectl exec'
      serviceAccountName: rook-ceph-system 
      containers:
      - name: fix-pools
        # Use a lightweight image with kubectl installed
        image: bitnami/kubectl:latest 
        env:
        - name: MON_POD
          # Command to find the name of the first running MON pod
          value: "$(kubectl get pod -l app=rook-ceph-mon -n rook-ceph --field-selector=status.phase=Running -o jsonpath='{.items[0].metadata.name}')"
        command: ["/bin/bash", "-c"]
        args:
          - |
            # 1. Wait for the Ceph cluster components to stabilize
            echo "Waiting 60 seconds for Ceph components to stabilize..."
            # sleep 60
            
            # 2. Define the Ceph CLI command wrapper
            CEPH_CMD="kubectl exec -n rook-ceph ${MON_POD} -- ceph"
            
            # 3. Apply the CRUSH rule change
            echo "Applying CRUSH rule and pool size fixes..."
            ${CEPH_CMD} osd crush rule create-replicated allow-single-host-osd-rule default osd || true
            
            # 4. Fix the .mgr pool
            ${CEPH_CMD} osd pool set .mgr crush_rule allow-single-host-osd-rule
            ${CEPH_CMD} osd pool set .mgr size 2
            ${CEPH_CMD} osd pool set .mgr min_size 1
            
            # 5. Fix the device_health_metrics pool
            ${CEPH_CMD} osd pool set device_health_metrics crush_rule allow-single-host-osd-rule
            ${CEPH_CMD} osd pool set device_health_metrics size 2
            ${CEPH_CMD} osd pool set device_health_metrics min_size 1

      restartPolicy: OnFailure
  backoffLimit: 5

