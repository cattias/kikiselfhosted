# --- 1. ROOK-CEPH OPERATOR BASE RESOURCES ---
# CRDs, Common, and Operator files are typically deployed first.
# This single file combines the necessary definitions for ArgoCD synchronization.
# You MUST ensure the official CRDs, common, and operator manifests are applied 
# by a separate Application or Kustomization base to avoid ArgoCD errors.
# For simplicity, this manifest focuses on the core CephCluster and StorageClass.
---
# --- 2. CEPHCLUSTER (The Brains) ---
apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: kiki-ceph
  namespace: rook-ceph
  annotations:
    argocd.argoproj.io/sync-wave: "1" 
spec:
  cephVersion:
    image: quay.io/ceph/ceph:v18.2.1
  dataDirHostPath: /var/lib/rook/ceph
  mon:
    count: 1 # Single monitor for single-node k0s (acceptable for homelab)
    allowMultiplePerNode: true
  mgr:
    count: 1
    allowMultiplePerNode: true
  network:
    # Use the host's network settings (critical for bare-metal performance)
    hostNetwork: true 
  storage:
    useAllNodes: true
    storageClassDeviceSets:
      - name: ssd-set
        count: 1
        portable: false
        volumeClaimTemplates: 
          - metadata:
              name: data
            spec:
              storageClassName: ""
              accessModes: ["ReadWriteOnce"]
              resources:
                requests:
                  storage: 1Gi
        # Rook will automatically discover and claim the 6 raw disks
        preparePlacement:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchLabels:
                      app: rook-ceph-osd
                  topologyKey: kubernetes.io/hostname
        config:
          # CRITICAL: UPDATED: Device filter from sda to sdf
          deviceFilter: "sda|sdb|sdc|sdd|sde|sdf"
          useAllDevices: "false" 
          osdsPerDevice: "1"
---
# --- 3. CEPH BLOCK POOL (The Redundancy Rule) ---
apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: ec-ssd-pool
  namespace: rook-ceph
  annotations:
    argocd.argoproj.io/sync-wave: "1"
spec:
  failureDomain: host
  erasureCode:
    dataChunks: 4    # k=4 (4 data chunks)
    codingChunks: 2  # m=2 (2 parity chunks - tolerates 2 drive failures)
  deviceClass: ssd
---
# --- 4. STORAGE CLASS (The K8s Interface) ---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: ceph-ssd-ec
  annotations:
    argocd.argoproj.io/sync-wave: "1"
provisioner: rook-ceph.rbd.csi.ceph.com
parameters:
  pool: ec-ssd-pool # Matches the pool defined above
  clusterID: rook-ceph
  imageFeatures: "layering"
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
  csi.storage.s.io/node-unstage-secret-name: rook-csi-rbd-node
  csi.storage.io/fstype: ext4 
reclaimPolicy: Delete
allowVolumeExpansion: true