# --- 1. ROOK-CEPH OPERATOR BASE RESOURCES ---
# CRDs, Common, and Operator files are typically deployed first.
# This single file combines the necessary definitions for ArgoCD synchronization.
# You MUST ensure the official CRDs, common, and operator manifests are applied 
# by a separate Application or Kustomization base to avoid ArgoCD errors.
# For simplicity, this manifest focuses on the core CephCluster and StorageClass.
---
# --- 2. CEPHCLUSTER (The Brains) ---
apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: kiki-ceph-cluster
  namespace: rook-ceph
  annotations:
    argocd.argoproj.io/sync-wave: "1" 
spec:
  cephVersion:
    image: quay.io/ceph/ceph:v18.2.1
  dataDirHostPath: /var/lib/rook/ceph
  mon:
    count: 1 # Single monitor for single-node k0s (acceptable for homelab)
    allowMultiplePerNode: true
  mgr:
    count: 1
    allowMultiplePerNode: true
  network:
    # Use the host's network settings (critical for bare-metal performance)
    hostNetwork: true 
  storage:
    useAllNodes: true
    storageClassDeviceSets:
      - name: ssd-set
        count: 1
        portable: false
        dataPVCTemplate:
          spec:
            storageClassName: "" # Use raw devices
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: "10Mi" # placeholder, size is determined by device
            volumeClaimTemplates:
              - metadata:
                  name: data
                spec:
                  storageClassName: ""
                  accessModes: ["ReadWriteOnce"]
                  resources:
                    requests:
                      storage: 1Gi
            # Rook will automatically discover and claim the 6 raw disks
            preparePlacement:
              podAntiAffinity:
                preferredDuringSchedulingIgnoredDuringExecution:
                  - weight: 100
                    podAffinityTerm:
                      labelSelector:
                        matchLabels:
                          app: rook-ceph-osd
                      topologyKey: kubernetes.io/hostname
            config:
              # CRITICAL: This tells Rook to pick up all unpartitioned drives (sda through sdf)
              deviceFilter: "sda|sdb|sdc|sdd|sde|sdf"
              useAllDevices: "false" 
              osdsPerDevice: "1"
---
# --- 3. CEPH BLOCK POOL (The Redundancy Rule) ---
apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: ec-ssd-pool
  namespace: rook-ceph
  annotations:
    argocd.argoproj.io/sync-wave: "1"
spec:
  failureDomain: host
  erasureCode:
    dataChunks: 4    # k=4 (4 data chunks)
    codingChunks: 2  # m=2 (2 parity chunks - tolerates 2 drive failures)
  deviceClass: ssd
---
# --- 4. STORAGE CLASS (The K8s Interface) ---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: ceph-ssd-ec
  annotations:
    argocd.argoproj.io/sync-wave: "1"
provisioner: rook-ceph.rbd.csi.ceph.com
parameters:
  pool: ec-ssd-pool # Matches the pool defined above
  clusterID: rook-ceph
  imageFeatures: "layering"
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
  csi.storage.s.io/node-unstage-secret-name: rook-csi-rbd-node
  csi.storage.io/fstype: ext4 
reclaimPolicy: Delete
allowVolumeExpansion: true
---
# --- 5. CEPH TOOLBOX (for debugging) ---
# This utility pod needs to wait for the core cluster to be UP and Secrets to be created.
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rook-ceph-tools
  namespace: rook-ceph
  annotations:
    argocd.argoproj.io/sync-wave: "2" # Runs AFTER the CephCluster is deployed
spec:
  selector:
    matchLabels:
      app: rook-ceph-tools
  template:
    metadata:
      labels:
        app: rook-ceph-tools
    spec:
      dnsPolicy: ClusterFirstWithHostNet # Required since the monitors use hostNetwork
      hostNetwork: true
      containers:
      - name: toolbox
        image: quay.io/ceph/ceph:v18.2.1
        command: ["/usr/local/bin/toolbox.sh"]
        imagePullPolicy: IfNotPresent
        env:
          - name: ROOK_CEPH_CLUSTER_FSID
            value: ""
        volumeMounts:
          - mountPath: /etc/ceph
            name: ceph-config
          - mountPath: /dev
            name: dev
          - mountPath: /sys/bus
            name: sysbus
          - mountPath: /lib/modules
            name: libmodules
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph
            name: ceph-lib
      volumes:
        - name: ceph-config
          secret:
            secretName: rook-ceph-mon # This Secret is created by the CephCluster (Wave 1)
        - name: dev
          hostPath:
            path: /dev
        - name: sysbus
          hostPath:
            path: /sys/bus
        - name: libmodules
          hostPath:
            path: /lib/modules
        - name: run-udev
          hostPath:
            path: /run/udev
        - name: ceph-lib
          hostPath:
            path: /var/lib/ceph
